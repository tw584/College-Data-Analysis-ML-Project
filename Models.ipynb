{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "```\n",
    "\n",
    "# 1. Data Overview\n",
    "\n",
    "\n",
    "```python\n",
    "df = pd.read_csv('college.csv')  \n",
    "df.head()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Unnamed: 0</th>\n",
    "      <th>Private</th>\n",
    "      <th>Apps</th>\n",
    "      <th>Accept</th>\n",
    "      <th>Enroll</th>\n",
    "      <th>Top10perc</th>\n",
    "      <th>Top25perc</th>\n",
    "      <th>F.Undergrad</th>\n",
    "      <th>P.Undergrad</th>\n",
    "      <th>Outstate</th>\n",
    "      <th>Room.Board</th>\n",
    "      <th>Books</th>\n",
    "      <th>Personal</th>\n",
    "      <th>PhD</th>\n",
    "      <th>Terminal</th>\n",
    "      <th>S.F.Ratio</th>\n",
    "      <th>perc.alumni</th>\n",
    "      <th>Expend</th>\n",
    "      <th>Grad.Rate</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>Abilene Christian University</td>\n",
    "      <td>Yes</td>\n",
    "      <td>1660</td>\n",
    "      <td>1232</td>\n",
    "      <td>721</td>\n",
    "      <td>23</td>\n",
    "      <td>52</td>\n",
    "      <td>2885</td>\n",
    "      <td>537</td>\n",
    "      <td>7440</td>\n",
    "      <td>3300</td>\n",
    "      <td>450</td>\n",
    "      <td>2200</td>\n",
    "      <td>70</td>\n",
    "      <td>78</td>\n",
    "      <td>18.1</td>\n",
    "      <td>12</td>\n",
    "      <td>7041</td>\n",
    "      <td>60</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>Adelphi University</td>\n",
    "      <td>Yes</td>\n",
    "      <td>2186</td>\n",
    "      <td>1924</td>\n",
    "      <td>512</td>\n",
    "      <td>16</td>\n",
    "      <td>29</td>\n",
    "      <td>2683</td>\n",
    "      <td>1227</td>\n",
    "      <td>12280</td>\n",
    "      <td>6450</td>\n",
    "      <td>750</td>\n",
    "      <td>1500</td>\n",
    "      <td>29</td>\n",
    "      <td>30</td>\n",
    "      <td>12.2</td>\n",
    "      <td>16</td>\n",
    "      <td>10527</td>\n",
    "      <td>56</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>Adrian College</td>\n",
    "      <td>Yes</td>\n",
    "      <td>1428</td>\n",
    "      <td>1097</td>\n",
    "      <td>336</td>\n",
    "      <td>22</td>\n",
    "      <td>50</td>\n",
    "      <td>1036</td>\n",
    "      <td>99</td>\n",
    "      <td>11250</td>\n",
    "      <td>3750</td>\n",
    "      <td>400</td>\n",
    "      <td>1165</td>\n",
    "      <td>53</td>\n",
    "      <td>66</td>\n",
    "      <td>12.9</td>\n",
    "      <td>30</td>\n",
    "      <td>8735</td>\n",
    "      <td>54</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>Agnes Scott College</td>\n",
    "      <td>Yes</td>\n",
    "      <td>417</td>\n",
    "      <td>349</td>\n",
    "      <td>137</td>\n",
    "      <td>60</td>\n",
    "      <td>89</td>\n",
    "      <td>510</td>\n",
    "      <td>63</td>\n",
    "      <td>12960</td>\n",
    "      <td>5450</td>\n",
    "      <td>450</td>\n",
    "      <td>875</td>\n",
    "      <td>92</td>\n",
    "      <td>97</td>\n",
    "      <td>7.7</td>\n",
    "      <td>37</td>\n",
    "      <td>19016</td>\n",
    "      <td>59</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>Alaska Pacific University</td>\n",
    "      <td>Yes</td>\n",
    "      <td>193</td>\n",
    "      <td>146</td>\n",
    "      <td>55</td>\n",
    "      <td>16</td>\n",
    "      <td>44</td>\n",
    "      <td>249</td>\n",
    "      <td>869</td>\n",
    "      <td>7560</td>\n",
    "      <td>4120</td>\n",
    "      <td>800</td>\n",
    "      <td>1500</td>\n",
    "      <td>76</td>\n",
    "      <td>72</td>\n",
    "      <td>11.9</td>\n",
    "      <td>2</td>\n",
    "      <td>10922</td>\n",
    "      <td>15</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "df.describe()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Apps</th>\n",
    "      <th>Accept</th>\n",
    "      <th>Enroll</th>\n",
    "      <th>Top10perc</th>\n",
    "      <th>Top25perc</th>\n",
    "      <th>F.Undergrad</th>\n",
    "      <th>P.Undergrad</th>\n",
    "      <th>Outstate</th>\n",
    "      <th>Room.Board</th>\n",
    "      <th>Books</th>\n",
    "      <th>Personal</th>\n",
    "      <th>PhD</th>\n",
    "      <th>Terminal</th>\n",
    "      <th>S.F.Ratio</th>\n",
    "      <th>perc.alumni</th>\n",
    "      <th>Expend</th>\n",
    "      <th>Grad.Rate</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>count</th>\n",
    "      <td>777.000000</td>\n",
    "      <td>777.000000</td>\n",
    "      <td>777.000000</td>\n",
    "      <td>777.000000</td>\n",
    "      <td>777.000000</td>\n",
    "      <td>777.000000</td>\n",
    "      <td>777.000000</td>\n",
    "      <td>777.000000</td>\n",
    "      <td>777.000000</td>\n",
    "      <td>777.000000</td>\n",
    "      <td>777.000000</td>\n",
    "      <td>777.000000</td>\n",
    "      <td>777.000000</td>\n",
    "      <td>777.000000</td>\n",
    "      <td>777.000000</td>\n",
    "      <td>777.000000</td>\n",
    "      <td>777.00000</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>mean</th>\n",
    "      <td>3001.638353</td>\n",
    "      <td>2018.804376</td>\n",
    "      <td>779.972973</td>\n",
    "      <td>27.558559</td>\n",
    "      <td>55.796654</td>\n",
    "      <td>3699.907336</td>\n",
    "      <td>855.298584</td>\n",
    "      <td>10440.669241</td>\n",
    "      <td>4357.526384</td>\n",
    "      <td>549.380952</td>\n",
    "      <td>1340.642214</td>\n",
    "      <td>72.660232</td>\n",
    "      <td>79.702703</td>\n",
    "      <td>14.089704</td>\n",
    "      <td>22.743887</td>\n",
    "      <td>9660.171171</td>\n",
    "      <td>65.46332</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>std</th>\n",
    "      <td>3870.201484</td>\n",
    "      <td>2451.113971</td>\n",
    "      <td>929.176190</td>\n",
    "      <td>17.640364</td>\n",
    "      <td>19.804778</td>\n",
    "      <td>4850.420531</td>\n",
    "      <td>1522.431887</td>\n",
    "      <td>4023.016484</td>\n",
    "      <td>1096.696416</td>\n",
    "      <td>165.105360</td>\n",
    "      <td>677.071454</td>\n",
    "      <td>16.328155</td>\n",
    "      <td>14.722359</td>\n",
    "      <td>3.958349</td>\n",
    "      <td>12.391801</td>\n",
    "      <td>5221.768440</td>\n",
    "      <td>17.17771</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>min</th>\n",
    "      <td>81.000000</td>\n",
    "      <td>72.000000</td>\n",
    "      <td>35.000000</td>\n",
    "      <td>1.000000</td>\n",
    "      <td>9.000000</td>\n",
    "      <td>139.000000</td>\n",
    "      <td>1.000000</td>\n",
    "      <td>2340.000000</td>\n",
    "      <td>1780.000000</td>\n",
    "      <td>96.000000</td>\n",
    "      <td>250.000000</td>\n",
    "      <td>8.000000</td>\n",
    "      <td>24.000000</td>\n",
    "      <td>2.500000</td>\n",
    "      <td>0.000000</td>\n",
    "      <td>3186.000000</td>\n",
    "      <td>10.00000</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>25%</th>\n",
    "      <td>776.000000</td>\n",
    "      <td>604.000000</td>\n",
    "      <td>242.000000</td>\n",
    "      <td>15.000000</td>\n",
    "      <td>41.000000</td>\n",
    "      <td>992.000000</td>\n",
    "      <td>95.000000</td>\n",
    "      <td>7320.000000</td>\n",
    "      <td>3597.000000</td>\n",
    "      <td>470.000000</td>\n",
    "      <td>850.000000</td>\n",
    "      <td>62.000000</td>\n",
    "      <td>71.000000</td>\n",
    "      <td>11.500000</td>\n",
    "      <td>13.000000</td>\n",
    "      <td>6751.000000</td>\n",
    "      <td>53.00000</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>50%</th>\n",
    "      <td>1558.000000</td>\n",
    "      <td>1110.000000</td>\n",
    "      <td>434.000000</td>\n",
    "      <td>23.000000</td>\n",
    "      <td>54.000000</td>\n",
    "      <td>1707.000000</td>\n",
    "      <td>353.000000</td>\n",
    "      <td>9990.000000</td>\n",
    "      <td>4200.000000</td>\n",
    "      <td>500.000000</td>\n",
    "      <td>1200.000000</td>\n",
    "      <td>75.000000</td>\n",
    "      <td>82.000000</td>\n",
    "      <td>13.600000</td>\n",
    "      <td>21.000000</td>\n",
    "      <td>8377.000000</td>\n",
    "      <td>65.00000</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>75%</th>\n",
    "      <td>3624.000000</td>\n",
    "      <td>2424.000000</td>\n",
    "      <td>902.000000</td>\n",
    "      <td>35.000000</td>\n",
    "      <td>69.000000</td>\n",
    "      <td>4005.000000</td>\n",
    "      <td>967.000000</td>\n",
    "      <td>12925.000000</td>\n",
    "      <td>5050.000000</td>\n",
    "      <td>600.000000</td>\n",
    "      <td>1700.000000</td>\n",
    "      <td>85.000000</td>\n",
    "      <td>92.000000</td>\n",
    "      <td>16.500000</td>\n",
    "      <td>31.000000</td>\n",
    "      <td>10830.000000</td>\n",
    "      <td>78.00000</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>max</th>\n",
    "      <td>48094.000000</td>\n",
    "      <td>26330.000000</td>\n",
    "      <td>6392.000000</td>\n",
    "      <td>96.000000</td>\n",
    "      <td>100.000000</td>\n",
    "      <td>31643.000000</td>\n",
    "      <td>21836.000000</td>\n",
    "      <td>21700.000000</td>\n",
    "      <td>8124.000000</td>\n",
    "      <td>2340.000000</td>\n",
    "      <td>6800.000000</td>\n",
    "      <td>103.000000</td>\n",
    "      <td>100.000000</td>\n",
    "      <td>39.800000</td>\n",
    "      <td>64.000000</td>\n",
    "      <td>56233.000000</td>\n",
    "      <td>118.00000</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "df.info()\n",
    "```\n",
    "\n",
    "    <class 'pandas.core.frame.DataFrame'>\n",
    "    RangeIndex: 777 entries, 0 to 776\n",
    "    Data columns (total 19 columns):\n",
    "     #   Column       Non-Null Count  Dtype  \n",
    "    ---  ------       --------------  -----  \n",
    "     0   Unnamed: 0   777 non-null    object \n",
    "     1   Private      777 non-null    object \n",
    "     2   Apps         777 non-null    int64  \n",
    "     3   Accept       777 non-null    int64  \n",
    "     4   Enroll       777 non-null    int64  \n",
    "     5   Top10perc    777 non-null    int64  \n",
    "     6   Top25perc    777 non-null    int64  \n",
    "     7   F.Undergrad  777 non-null    int64  \n",
    "     8   P.Undergrad  777 non-null    int64  \n",
    "     9   Outstate     777 non-null    int64  \n",
    "     10  Room.Board   777 non-null    int64  \n",
    "     11  Books        777 non-null    int64  \n",
    "     12  Personal     777 non-null    int64  \n",
    "     13  PhD          777 non-null    int64  \n",
    "     14  Terminal     777 non-null    int64  \n",
    "     15  S.F.Ratio    777 non-null    float64\n",
    "     16  perc.alumni  777 non-null    int64  \n",
    "     17  Expend       777 non-null    int64  \n",
    "     18  Grad.Rate    777 non-null    int64  \n",
    "    dtypes: float64(1), int64(16), object(2)\n",
    "    memory usage: 115.5+ KB\n",
    "    \n",
    "\n",
    "# 2. Data Processing\n",
    "\n",
    "# 3. Model\n",
    "\n",
    "## 3.1 Random Forest\n",
    "\n",
    "\n",
    "```python\n",
    "data['Elite'].sum()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    112\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "data['Private_numeric'] = np.where(data['Private'].str.lower() == 'yes', 1, 0)\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "data\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Unnamed: 0</th>\n",
    "      <th>Private</th>\n",
    "      <th>Apps</th>\n",
    "      <th>Accept</th>\n",
    "      <th>Enroll</th>\n",
    "      <th>Top10perc</th>\n",
    "      <th>Top25perc</th>\n",
    "      <th>F.Undergrad</th>\n",
    "      <th>P.Undergrad</th>\n",
    "      <th>Outstate</th>\n",
    "      <th>...</th>\n",
    "      <th>Books</th>\n",
    "      <th>Personal</th>\n",
    "      <th>PhD</th>\n",
    "      <th>Terminal</th>\n",
    "      <th>S.F.Ratio</th>\n",
    "      <th>perc.alumni</th>\n",
    "      <th>Expend</th>\n",
    "      <th>Grad.Rate</th>\n",
    "      <th>Elite</th>\n",
    "      <th>Private_numeric</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>Abilene Christian University</td>\n",
    "      <td>Yes</td>\n",
    "      <td>1660</td>\n",
    "      <td>1232</td>\n",
    "      <td>721</td>\n",
    "      <td>23</td>\n",
    "      <td>52</td>\n",
    "      <td>2885</td>\n",
    "      <td>537</td>\n",
    "      <td>7440</td>\n",
    "      <td>...</td>\n",
    "      <td>450</td>\n",
    "      <td>2200</td>\n",
    "      <td>70</td>\n",
    "      <td>78</td>\n",
    "      <td>18.1</td>\n",
    "      <td>12</td>\n",
    "      <td>7041</td>\n",
    "      <td>60</td>\n",
    "      <td>0</td>\n",
    "      <td>1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>Adelphi University</td>\n",
    "      <td>Yes</td>\n",
    "      <td>2186</td>\n",
    "      <td>1924</td>\n",
    "      <td>512</td>\n",
    "      <td>16</td>\n",
    "      <td>29</td>\n",
    "      <td>2683</td>\n",
    "      <td>1227</td>\n",
    "      <td>12280</td>\n",
    "      <td>...</td>\n",
    "      <td>750</td>\n",
    "      <td>1500</td>\n",
    "      <td>29</td>\n",
    "      <td>30</td>\n",
    "      <td>12.2</td>\n",
    "      <td>16</td>\n",
    "      <td>10527</td>\n",
    "      <td>56</td>\n",
    "      <td>0</td>\n",
    "      <td>1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>Adrian College</td>\n",
    "      <td>Yes</td>\n",
    "      <td>1428</td>\n",
    "      <td>1097</td>\n",
    "      <td>336</td>\n",
    "      <td>22</td>\n",
    "      <td>50</td>\n",
    "      <td>1036</td>\n",
    "      <td>99</td>\n",
    "      <td>11250</td>\n",
    "      <td>...</td>\n",
    "      <td>400</td>\n",
    "      <td>1165</td>\n",
    "      <td>53</td>\n",
    "      <td>66</td>\n",
    "      <td>12.9</td>\n",
    "      <td>30</td>\n",
    "      <td>8735</td>\n",
    "      <td>54</td>\n",
    "      <td>0</td>\n",
    "      <td>1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>Agnes Scott College</td>\n",
    "      <td>Yes</td>\n",
    "      <td>417</td>\n",
    "      <td>349</td>\n",
    "      <td>137</td>\n",
    "      <td>60</td>\n",
    "      <td>89</td>\n",
    "      <td>510</td>\n",
    "      <td>63</td>\n",
    "      <td>12960</td>\n",
    "      <td>...</td>\n",
    "      <td>450</td>\n",
    "      <td>875</td>\n",
    "      <td>92</td>\n",
    "      <td>97</td>\n",
    "      <td>7.7</td>\n",
    "      <td>37</td>\n",
    "      <td>19016</td>\n",
    "      <td>59</td>\n",
    "      <td>0</td>\n",
    "      <td>1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>Alaska Pacific University</td>\n",
    "      <td>Yes</td>\n",
    "      <td>193</td>\n",
    "      <td>146</td>\n",
    "      <td>55</td>\n",
    "      <td>16</td>\n",
    "      <td>44</td>\n",
    "      <td>249</td>\n",
    "      <td>869</td>\n",
    "      <td>7560</td>\n",
    "      <td>...</td>\n",
    "      <td>800</td>\n",
    "      <td>1500</td>\n",
    "      <td>76</td>\n",
    "      <td>72</td>\n",
    "      <td>11.9</td>\n",
    "      <td>2</td>\n",
    "      <td>10922</td>\n",
    "      <td>15</td>\n",
    "      <td>0</td>\n",
    "      <td>1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>...</th>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>772</th>\n",
    "      <td>Worcester State College</td>\n",
    "      <td>No</td>\n",
    "      <td>2197</td>\n",
    "      <td>1515</td>\n",
    "      <td>543</td>\n",
    "      <td>4</td>\n",
    "      <td>26</td>\n",
    "      <td>3089</td>\n",
    "      <td>2029</td>\n",
    "      <td>6797</td>\n",
    "      <td>...</td>\n",
    "      <td>500</td>\n",
    "      <td>1200</td>\n",
    "      <td>60</td>\n",
    "      <td>60</td>\n",
    "      <td>21.0</td>\n",
    "      <td>14</td>\n",
    "      <td>4469</td>\n",
    "      <td>40</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>773</th>\n",
    "      <td>Xavier University</td>\n",
    "      <td>Yes</td>\n",
    "      <td>1959</td>\n",
    "      <td>1805</td>\n",
    "      <td>695</td>\n",
    "      <td>24</td>\n",
    "      <td>47</td>\n",
    "      <td>2849</td>\n",
    "      <td>1107</td>\n",
    "      <td>11520</td>\n",
    "      <td>...</td>\n",
    "      <td>600</td>\n",
    "      <td>1250</td>\n",
    "      <td>73</td>\n",
    "      <td>75</td>\n",
    "      <td>13.3</td>\n",
    "      <td>31</td>\n",
    "      <td>9189</td>\n",
    "      <td>83</td>\n",
    "      <td>1</td>\n",
    "      <td>1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>774</th>\n",
    "      <td>Xavier University of Louisiana</td>\n",
    "      <td>Yes</td>\n",
    "      <td>2097</td>\n",
    "      <td>1915</td>\n",
    "      <td>695</td>\n",
    "      <td>34</td>\n",
    "      <td>61</td>\n",
    "      <td>2793</td>\n",
    "      <td>166</td>\n",
    "      <td>6900</td>\n",
    "      <td>...</td>\n",
    "      <td>617</td>\n",
    "      <td>781</td>\n",
    "      <td>67</td>\n",
    "      <td>75</td>\n",
    "      <td>14.4</td>\n",
    "      <td>20</td>\n",
    "      <td>8323</td>\n",
    "      <td>49</td>\n",
    "      <td>0</td>\n",
    "      <td>1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>775</th>\n",
    "      <td>Yale University</td>\n",
    "      <td>Yes</td>\n",
    "      <td>10705</td>\n",
    "      <td>2453</td>\n",
    "      <td>1317</td>\n",
    "      <td>95</td>\n",
    "      <td>99</td>\n",
    "      <td>5217</td>\n",
    "      <td>83</td>\n",
    "      <td>19840</td>\n",
    "      <td>...</td>\n",
    "      <td>630</td>\n",
    "      <td>2115</td>\n",
    "      <td>96</td>\n",
    "      <td>96</td>\n",
    "      <td>5.8</td>\n",
    "      <td>49</td>\n",
    "      <td>40386</td>\n",
    "      <td>99</td>\n",
    "      <td>1</td>\n",
    "      <td>1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>776</th>\n",
    "      <td>York College of Pennsylvania</td>\n",
    "      <td>Yes</td>\n",
    "      <td>2989</td>\n",
    "      <td>1855</td>\n",
    "      <td>691</td>\n",
    "      <td>28</td>\n",
    "      <td>63</td>\n",
    "      <td>2988</td>\n",
    "      <td>1726</td>\n",
    "      <td>4990</td>\n",
    "      <td>...</td>\n",
    "      <td>500</td>\n",
    "      <td>1250</td>\n",
    "      <td>75</td>\n",
    "      <td>75</td>\n",
    "      <td>18.1</td>\n",
    "      <td>28</td>\n",
    "      <td>4509</td>\n",
    "      <td>99</td>\n",
    "      <td>1</td>\n",
    "      <td>1</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "<p>777 rows × 21 columns</p>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "\n",
    "# load the data\n",
    "data = pd.read_csv(\"College.csv\")\n",
    "\n",
    "# preprocessing\n",
    "data['Private_numeric'] = np.where(data['Private'].str.lower() == 'yes', 1, 0)\n",
    "\n",
    "median_apps = data['Apps'].median()\n",
    "median_alumni = data['perc.alumni'].median()\n",
    "\n",
    "data['Elite'] = ((data['Grad.Rate'] > 75) & \n",
    "                 (data['Apps'] > median_apps) & \n",
    "                 (data['perc.alumni'] > median_alumni)).astype(int)\n",
    "\n",
    "# fetures and target variable\n",
    "features = ['Accept', 'Enroll', 'Top10perc', 'F.Undergrad', \n",
    "            'Outstate', 'Room.Board', 'Books', 'Personal', \n",
    "            'PhD', 'Terminal', 'S.F.Ratio', 'Expend', 'Private_numeric']\n",
    "X = data[features]\n",
    "y = data['Elite']\n",
    "\n",
    "# split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# random forest \n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# train the data\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# prediction\n",
    "y_pred = rf_model.predict(X_test)\n",
    "y_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# feature importance\n",
    "importances = rf_model.feature_importances_\n",
    "feature_importance = pd.DataFrame({'Feature': features, 'Importance': importances}).sort_values(by='Importance', ascending=False)\n",
    "print(\"\\nFeature Importance:\\n\", feature_importance)\n",
    "\n",
    "```\n",
    "\n",
    "    Accuracy: 0.9273504273504274\n",
    "    ROC AUC: 0.9595258136924804\n",
    "    Classification Report:\n",
    "                   precision    recall  f1-score   support\n",
    "    \n",
    "               0       0.93      0.98      0.96       198\n",
    "               1       0.88      0.61      0.72        36\n",
    "    \n",
    "        accuracy                           0.93       234\n",
    "       macro avg       0.91      0.80      0.84       234\n",
    "    weighted avg       0.92      0.93      0.92       234\n",
    "    \n",
    "    \n",
    "    Feature Importance:\n",
    "                 Feature  Importance\n",
    "    4          Outstate    0.184025\n",
    "    2         Top10perc    0.122649\n",
    "    0            Accept    0.097213\n",
    "    11           Expend    0.086384\n",
    "    1            Enroll    0.086366\n",
    "    3       F.Undergrad    0.083317\n",
    "    5        Room.Board    0.081015\n",
    "    7          Personal    0.060401\n",
    "    10        S.F.Ratio    0.058595\n",
    "    9          Terminal    0.049574\n",
    "    8               PhD    0.048563\n",
    "    6             Books    0.027065\n",
    "    12  Private_numeric    0.014833\n",
    "    \n",
    "\n",
    "## 3.2 Logistic Regression\n",
    "\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# load the data\n",
    "data = pd.read_csv(\"College.csv\")\n",
    "\n",
    "# proprocessing\n",
    "data['Private_numeric'] = np.where(data['Private'].str.lower() == 'yes', 1, 0)\n",
    "\n",
    "# binary variable\n",
    "median_apps = data['Apps'].median()\n",
    "median_alumni = data['perc.alumni'].median()\n",
    "\n",
    "data['Elite'] = ((data['Grad.Rate'] > 75) & \n",
    "                 (data['Apps'] > median_apps) & \n",
    "                 (data['perc.alumni'] > median_alumni)).astype(int)\n",
    "\n",
    "# features and target variable\n",
    "features = ['Accept', 'Enroll', 'Top10perc', 'F.Undergrad', \n",
    "            'Outstate', 'Room.Board', 'Books', 'Personal', \n",
    "            'PhD', 'Terminal', 'S.F.Ratio', 'Expend', 'Private_numeric']\n",
    "X = data[features]\n",
    "y = data['Elite']\n",
    "\n",
    "# split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# train and predict\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# logistic\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, classification_report, accuracy_score\n",
    "\n",
    "log_reg_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_reg_model.fit(X_train_scaled, y_train)\n",
    "y_pred_log = log_reg_model.predict(X_test_scaled)\n",
    "y_pred_proba_log = log_reg_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# evaluation\n",
    "accuracy_log = accuracy_score(y_test, y_pred_log)\n",
    "roc_auc_log = roc_auc_score(y_test, y_pred_proba_log)\n",
    "\n",
    "print(\"Logistic Regression - Accuracy:\", accuracy_log)\n",
    "print(\"Logistic Regression - ROC AUC:\", roc_auc_log)\n",
    "print(\"Logistic Regression - Classification Report:\\n\", classification_report(y_test, y_pred_log))\n",
    "\n",
    "```\n",
    "\n",
    "    Logistic Regression - Accuracy: 0.9102564102564102\n",
    "    Logistic Regression - ROC AUC: 0.9395342312008979\n",
    "    Logistic Regression - Classification Report:\n",
    "                   precision    recall  f1-score   support\n",
    "    \n",
    "               0       0.91      0.99      0.95       198\n",
    "               1       0.94      0.44      0.60        36\n",
    "    \n",
    "        accuracy                           0.91       234\n",
    "       macro avg       0.92      0.72      0.78       234\n",
    "    weighted avg       0.91      0.91      0.90       234\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Getting the coefficients from the logistic regression model\n",
    "feature_importance = log_reg_model.coef_[0]\n",
    "\n",
    "# If you have a DataFrame of features, you can match the feature names\n",
    "features = pd.DataFrame(feature_importance, index=X_train.columns, columns=[\"Coefficient\"])\n",
    "features = features.abs().sort_values(by=\"Coefficient\", ascending=False)\n",
    "print(features)\n",
    "\n",
    "```\n",
    "\n",
    "                     Coefficient\n",
    "    Outstate            2.767891\n",
    "    Top10perc           2.155170\n",
    "    Personal            1.217562\n",
    "    PhD                 1.176484\n",
    "    Terminal            1.171240\n",
    "    Room.Board          0.620127\n",
    "    Private_numeric     0.517095\n",
    "    Expend              0.351488\n",
    "    Books               0.351173\n",
    "    Enroll              0.297975\n",
    "    F.Undergrad         0.295728\n",
    "    Accept              0.113923\n",
    "    S.F.Ratio           0.055213\n",
    "    \n",
    "\n",
    "\n",
    "```python\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# import constant\n",
    "X_train_sm = sm.add_constant(X_train)\n",
    "\n",
    "# use Statsmodels \n",
    "logit_model = sm.Logit(y_train, X_train_sm)\n",
    "result = logit_model.fit()\n",
    "\n",
    "# output\n",
    "print(result.summary())\n",
    "\n",
    "```\n",
    "\n",
    "    Optimization terminated successfully.\n",
    "             Current function value: 0.253343\n",
    "             Iterations 8\n",
    "                               Logit Regression Results                           \n",
    "    ==============================================================================\n",
    "    Dep. Variable:                  Elite   No. Observations:                  543\n",
    "    Model:                          Logit   Df Residuals:                      529\n",
    "    Method:                           MLE   Df Model:                           13\n",
    "    Date:                Sun, 01 Dec 2024   Pseudo R-squ.:                  0.3743\n",
    "    Time:                        19:19:37   Log-Likelihood:                -137.57\n",
    "    converged:                       True   LL-Null:                       -219.86\n",
    "    Covariance Type:            nonrobust   LLR p-value:                 2.319e-28\n",
    "    ===================================================================================\n",
    "                          coef    std err          z      P>|z|      [0.025      0.975]\n",
    "    -----------------------------------------------------------------------------------\n",
    "    const              -8.9741      2.132     -4.209      0.000     -13.153      -4.795\n",
    "    Accept           -4.03e-05      0.000     -0.182      0.855      -0.000       0.000\n",
    "    Enroll              0.0027      0.001      2.732      0.006       0.001       0.005\n",
    "    Top10perc           0.0336      0.011      2.928      0.003       0.011       0.056\n",
    "    F.Undergrad        -0.0006      0.000     -2.310      0.021      -0.001   -8.64e-05\n",
    "    Outstate            0.0003   7.03e-05      3.664      0.000       0.000       0.000\n",
    "    Room.Board       2.188e-05      0.000      0.119      0.906      -0.000       0.000\n",
    "    Books               0.0010      0.001      0.996      0.319      -0.001       0.003\n",
    "    Personal           -0.0006      0.000     -1.721      0.085      -0.001    8.21e-05\n",
    "    PhD                 0.0154      0.023      0.663      0.507      -0.030       0.061\n",
    "    Terminal            0.0169      0.029      0.586      0.558      -0.040       0.073\n",
    "    S.F.Ratio           0.0385      0.063      0.611      0.541      -0.085       0.162\n",
    "    Expend          -2.912e-05   3.84e-05     -0.758      0.449      -0.000    4.62e-05\n",
    "    Private_numeric    -0.0667      0.763     -0.088      0.930      -1.561       1.428\n",
    "    ===================================================================================\n",
    "    \n",
    "\n",
    "## 3.3 Model Comparison\n",
    "\n",
    "\n",
    "```python\n",
    "# Print results for Random Forest\n",
    "print(\"Random Forest - Accuracy:\", accuracy)\n",
    "print(\"Random Forest - ROC AUC:\", roc_auc)\n",
    "\n",
    "# Print results for Logistic Regression\n",
    "print(\"Logistic Regression - Accuracy:\", accuracy_log)\n",
    "print(\"Logistic Regression - ROC AUC:\", roc_auc_log)\n",
    "\n",
    "# Visualize the ROC curves for both models\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Random Forest ROC Curve\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_proba)\n",
    "# Logistic Regression ROC Curve\n",
    "fpr_log, tpr_log, _ = roc_curve(y_test, y_pred_proba_log)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr_rf, tpr_rf, label='Random Forest (AUC = {:.2f})'.format(roc_auc))\n",
    "plt.plot(fpr_log, tpr_log, label='Logistic Regression (AUC = {:.2f})'.format(roc_auc_log))\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Chance')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "```\n",
    "\n",
    "    Random Forest - Accuracy: 0.9145299145299145\n",
    "    Random Forest - ROC AUC: 0.9560185185185185\n",
    "    Logistic Regression - Accuracy: 0.8974358974358975\n",
    "    Logistic Regression - ROC AUC: 0.9344837261503929\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "![png](output_17_1.png)\n",
    "    \n",
    "\n",
    "\n",
    "## 3.4 Robustness\n",
    "\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Cross-validation for Random Forest\n",
    "rf_cv_scores = cross_val_score(rf_model, X, y, cv=5, scoring='accuracy')\n",
    "print(\"Random Forest - Cross-Validation Accuracy:\", rf_cv_scores)\n",
    "print(\"Random Forest - Mean CV Accuracy:\", rf_cv_scores.mean())\n",
    "\n",
    "# Cross-validation for Logistic Regression\n",
    "log_reg_cv_scores = cross_val_score(log_reg_model, X, y, cv=5, scoring='accuracy')\n",
    "print(\"Logistic Regression - Cross-Validation Accuracy:\", log_reg_cv_scores)\n",
    "print(\"Logistic Regression - Mean CV Accuracy:\", log_reg_cv_scores.mean())\n",
    "\n",
    "```\n",
    "\n",
    "    Random Forest - Cross-Validation Accuracy: [0.91025641 0.94230769 0.92258065 0.86451613 0.94193548]\n",
    "    Random Forest - Mean CV Accuracy: 0.9163192721257236\n",
    "    \n",
    "\n",
    "    D:\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
    "    STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
    "    \n",
    "    Increase the number of iterations (max_iter) or scale the data as shown in:\n",
    "        https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "    Please also refer to the documentation for alternative solver options:\n",
    "        https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "      n_iter_i = _check_optimize_result(\n",
    "    D:\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
    "    STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
    "    \n",
    "    Increase the number of iterations (max_iter) or scale the data as shown in:\n",
    "        https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "    Please also refer to the documentation for alternative solver options:\n",
    "        https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "      n_iter_i = _check_optimize_result(\n",
    "    D:\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
    "    STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
    "    \n",
    "    Increase the number of iterations (max_iter) or scale the data as shown in:\n",
    "        https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "    Please also refer to the documentation for alternative solver options:\n",
    "        https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "      n_iter_i = _check_optimize_result(\n",
    "    D:\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
    "    STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
    "    \n",
    "    Increase the number of iterations (max_iter) or scale the data as shown in:\n",
    "        https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "    Please also refer to the documentation for alternative solver options:\n",
    "        https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "      n_iter_i = _check_optimize_result(\n",
    "    \n",
    "\n",
    "    Logistic Regression - Cross-Validation Accuracy: [0.89102564 0.8974359  0.88387097 0.83225806 0.91612903]\n",
    "    Logistic Regression - Mean CV Accuracy: 0.8841439205955336\n",
    "    \n",
    "\n",
    "    D:\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
    "    STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
    "    \n",
    "    Increase the number of iterations (max_iter) or scale the data as shown in:\n",
    "        https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "    Please also refer to the documentation for alternative solver options:\n",
    "        https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "      n_iter_i = _check_optimize_result(\n",
    "    \n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
